# 参数学习(以线性回归为例)

- 经验风险最小化(最小二乘法)
- 结构风险最小化(岭回归)
- 最大似然估计
- 最大后验估计

## 最小二乘法(Least Square Method, LSM)

使用平方损失函数 MSE 作为经验风险最小化的损失

$$
\begin{aligned}
\mathcal{R}(w) &= \sum _{n = 1} ^{N} \mathcal{L}(y^{(n)}, f(x^{(n)}; w)) \\
&= \frac{1}{2} \sum _{n = 1} ^{N} (y^{(n)} - w ^{T}x^{(n)}) ^{2} \\
&= \frac{1}{2} || y - X ^{T}w || ^{2}
\end{aligned}
$$

其中 $y$ 是所有样本标签构成的列向量，$X$ 是所有输入样本特征构成的矩阵，损失对参数 $w$ 求偏导得

$$
\begin{aligned}
\frac{\partial \mathcal{R}(w)}{\partial w} &= \frac{1}{2} \frac{\partial || y - X ^{T}w || ^{2}}{\partial w} \\
&= - X(y - X ^{T}w)
\end{aligned}
$$

令 $\frac{\partial}{\partial w} \mathcal{R} (w)$, 得到最优的参数 $w ^{*}$ 为

> $$
> w ^{*} = (X X ^{T}) ^{-1} X y
> $$

## 结构风险最小化(岭回归, Ridge Regression)

使用 LSM 需要保证特征之间互相独立，即 $XX ^{T}$ 可逆。但即使 $XX^{T}$ 可逆，特征之间如果存在线性相关性，仍然会使得 LSM 的计算不稳定。而岭回归则给 $XX^{T}$ 对角线元素加了一个常数, 使 $(XX^{T} + \lambda I)$ 满秩，其最优参数 $w ^{*}$ 为

$$
w ^{*} = (XX^{T} + \lambda I) ^{-1} X y
$$

岭回归也可以看做是结构风险准则下的最小二乘估计(带正则化项)，目标函数为

$$
\mathcal{R}(w) = \frac{1}{2} || y - X ^{T}w || ^{2} + \frac{\lambda}{2} || w || ^{2}
$$

## 最大似然估计(Maximum Likelihood Estimation, MLE)

> 从条件概率 $p(y|x)$ 建模

假设 $y$ 为一个随机变量，由函数 $f(x; w) = w^{T}x$ 加上一个随机噪声，其中噪声 $\epsilon$ 服从均值 0 ，方差 $\sigma ^{2}$ 的高斯分布，这样目标 $y$ 服从均值 $w^{T}x$，方差 $\sigma ^{2}$ 的高斯分布

$$
\begin{aligned}
  p(y|x; w) &= \mathcal{N}(y; w^{T}x, \sigma ^{2}) \\
  &= \frac{1}{\sqrt{2 \pi \sigma ^{2}}} \exp \left( - \frac{(y - w^{T}x) ^{2}}{2 \sigma ^{2}} \right)
\end{aligned}
$$

参数 $w$ 在训练集上的似然函数为

$$
\begin{aligned}
  p(y|x; w, \sigma) &= \prod _{n = 1} ^{N} p(y^{(n)}|x^{(n)}; w, \sigma) \\
  &= \prod _{n = 1} ^{N} \mathcal{N}(y ^{(n)}; w^{T}x^{(n)}, \sigma ^{2})
\end{aligned}
$$

其对数似然函数(Log Likelihood Function)为:

$$
\log p(y|x; w, \sigma) = \sum _{n = 1} ^{N} \log \mathcal{N}(y ^{(n)}; w^{T}x^{(n)}, \sigma ^{2})
$$

最大似然估计MLE即是找到一组参数 $w$ 使得似然函数$p(y|X; w, \sigma)$最大, 等价于对数似然函数$\log p(y|X; w, \sigma)$最大.

$$
\text{令} \frac{\partial \log p(y|X; w, \sigma)}{\partial w} = 0 \\
\Downarrow \\
w ^{ML} = (X X ^{T}) ^{-1} X y
$$

MLE 可以得到和最小二乘法一样的结果。

## 最大后验估计(Maximum A Posteriori Estimation, MAP)

为了避免过拟合，可以给参数加上一些先验知识。假设参数 $w$ 为一个随机向量，并服从一个先验分布 $p(w; v)$, 令 $p(w; v)$ 为各向同性高斯分布 $p(w; v) = \mathcal{N}(w; 0, v^{2}I)$, 其中 $v^{2}$ 为方差。

根据贝叶斯公式，参数 $w$ 的后验分布为

$$
\begin{aligned}
  p(w|X, y; v, \sigma) &= \frac{p(w, y|X; v, \sigma)}{\sum _{w} p(w, y|X; v, \sigma)} \\
  & \propto p(y|X, w; \sigma) p(w; v)
\end{aligned}
$$

这种估计参数 $w$ 的后验概率分布的方法为贝叶斯估计，贝叶斯估计是一种参数的区间估计，如果要得到一个最优的参数值（点估计）可以使用最大后验估计 MAP ，即最优参数为后验分布 $p(w|X, y; v, \sigma)$ 中密度最高的参数

$$
w ^{MAP} = \arg \max _{w} p(w|X, y; v, \sigma)p(w; v)
$$

似然函数满足之前推导的高斯密度函数

$$
\begin{aligned}
  p(y|X, w; \sigma) &= \prod _{n = 1} ^{N} p(y^{(n)}|x^{(n)}, w; \sigma) \\
  &= \prod _{n = 1} ^{N} \mathcal{N}(y ^{(n)}; w^{T}x^{(n)}, \sigma ^{2}) \\
\end{aligned}
$$

其对数似然为

$$
\begin{aligned}
  \log p(w|X, y; v, \sigma) & \propto \log p(y|X, w; \sigma) + \log p(w; v) \\
  & \propto - \frac{1}{2 \sigma ^{2}} \sum _{n = 1} ^{N} (y ^{(n)} - w^{T}x^{(n)}) ^{2} - \frac{1}{2v^{2}} w ^{T}w \\
  &= - \frac{1}{2 \sigma ^{2}} || y - X ^{T}w || ^{2} - \frac{1}{2v^{2}} w^{T}w
\end{aligned}
$$

其中正则化系数为 $\lambda = \frac{\sigma ^{2}}{v ^{2}}$

## 参数学习总结

|  | 无先验 | 引入先验 |
| :---: | :---: | :---: |
| 平方误差 | 经验风险最小化 | 结构风险最小化 |
| 概率 | 最大似然估计 | 最大后验估计 |
|  | $w^{ML} = (XX^{T})^{-1}Xy$ | $w ^{*} = (XX^{T} + \lambda I)^{-1}Xy$ |
