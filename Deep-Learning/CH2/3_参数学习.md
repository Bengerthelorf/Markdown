# 参数学习

通过一个简单的模型(线性回归)来具体了解机器学习的一般过程，以及不同学习准则(经验风险最小化、结构风险最小化、最大似然估计、最大后验估计)之间的关系.

线性回归(Linear Regression)是机器学习和统计学中最基础和最广泛应用的模型，是一种对自变量和因变量之间关系进行建模的回归分析.自变量数量为1时称为简单回归，自变量数量大于1时称为多元回归.从机器学习的角度来看，自变量就是样本的特征向量x∈R^D (每一维对应一个自变量)，因变量是标签𝑦，这里𝑦∈R是连续值(实数或连续整数).假设空间是一组参数化的线性函数

$$
f(x; w, b) = w^{T}x + b
$$

其中权重向量w∈R^D和偏置𝑏∈R都是可学习的参数，函数𝑓(𝒙;𝒘,𝑏)∈R也称为线性模型.给定一组包含𝑁个训练样本的训练集𝒟={(x^((n) ),y^((n) ) )}_(n=1)^N，我们希望能够学习一个最优的线性回归的模型参数𝒘.

## 5.1. 经验风险最小化

由于线性回归的标签𝑦和模型输出都为连续的实数值，因此平方损失函数非常合适衡量真实标签和预测标签之间的差异.根据经验风险最小化准则，训练集𝒟上的经验风险定义为

$$
\begin{aligned}
    \mathcal{R}(w) &= \sum _{n = 1} ^{N} \mathcal{L}(y^{(n)}, f(x^{(n)}; w)) \\
    &= \frac{1}{2} \sum _{n = 1} ^{N} (y^{(n)} - w ^{T}x^{(n)}) ^{2} \\
    &= \frac{1}{2} || y - X ^{T}w || ^{2}
\end{aligned}
$$

其中𝒚是由所有样本的真实标签组成的列向量，而𝑿是由所有样本的输入特征x^((1))…x^((N))组成的矩阵. 风险函数R(𝒘)是关于𝒘的凸函数，其对𝒘的偏导数为

$$
\begin{aligned}
    \frac{\partial \mathcal{R}(w)}{\partial w} &= \frac{1}{2} \frac{\partial || y - X ^{T}w || ^{2}}{\partial w} \\
    &= - X(y - X ^{T}w)
\end{aligned}
$$

令(∂R(w))/∂w=0，可以得到最优参数w^*为

$$
\begin{aligned}
    w ^{*} &= (X X ^{T}) ^{-1} X y \\
    &= (\sum _{n = 1} ^{N} x^{(n)} x^{(n) T}) ^{-1} \sum _{n = 1} ^{N} x^{(n)} y^{(n)}
\end{aligned}
$$

这种求解线性回归参数的方法也叫最小二乘法(Least Square Method，LSM).在最小二乘法中，XX^T必须存在逆矩阵，即XX^T是满秩的.也就是说，𝑿中的行向量之间是线性不相关的，即每一个特征都和其他特征不相关.一种常见的XX^T不可逆情况是样本数量𝑁小于特征数量(𝐷+1)，XX^T的秩为𝑁.这时会存在很多解𝒘∗，可以使得R(𝒘∗)=0.当XX^T不可逆时，可以通过下面两种方法来估计参数:

1. 先使用主成分分析等方法来预处理数据，消除不同特征之间的相关性，然后再使用最小二乘法来估计参数;
2. 使用梯度下降法来估计参数.

## 5.2. 结构风险最小化

最小二乘法的基本要求是各个特征之间要互相独立，保证XX^T可逆.但即使XX^T可逆，如果特征之间有较大的多重共线性(Multicollinearity)，也会使得XX^T的逆在数值上无法准确计算.数据集𝑿上一些小的扰动就会导致(X〖X^T)〗^(-1)发生大的改变，进而使得最小二乘法的计算变得很不稳定.为了解决这个问题，[Hoerletal.,1970]提出了岭回归(Ridge Regression)，给XX^T的对角线元素都加上一个常数𝜆，使得(XX^T+𝜆𝐼)满秩，即其行列式不为0.最优的参数𝒘∗为

$$
w ^{*} = (XX^{T} + \lambda I) ^{-1} X y
$$

其中 $\lambda > 0$ 为预先设置的超参数，$I$ 为单位矩阵.岭回归的解 $w ^{*}$ 可以看作结构风险最小化准则下的最小二乘法估计，其目标函数可以写为

$$
\mathcal{R}(w) = \frac{1}{2} || y - X ^{T}w || ^{2} + \frac{1}{2} \lambda || w || ^{2}
$$

其中λ为正则化系数。

## 5.3. 最大似然估计

线性回归还可以从建模条件概率𝑝(𝑦|𝒙)的角度来进行参数估计. 假设标签𝑦为一个随机变量，并由函数𝑓(𝒙;𝒘)=w^T x加上一个随机噪声𝜖决定，其中𝜖服从均值为0、方差为σ^2的高斯分布.这样，𝑦服从均值为w^T x、方差为σ^2的高斯分布

$$
\begin{aligned}
    p(y|X; w, \sigma) &= \prod _{n = 1} ^{N} p(y^{(n)}|x^{(n)}; w, \sigma) \\
    &= \prod _{n = 1} ^{N} \mathcal{N}(y^{(n)}; w^{T}x^{(n)}, \sigma ^{2}) \\
\end{aligned}
$$

为了方便计算，对似然函数取对数得到对数似然函数(Log Likelihood)

$$
\log p(y|X; w, \sigma) = \sum _{n = 1} ^{N} \log \mathcal{N}(y^{(n)}; w^{T}x^{(n)}, \sigma ^{2})
$$

最大似然估计(Maximum Likelihood Estimation，MLE)是指找到一组参数𝒘使得似然函数𝑝(𝒚|𝑿;𝒘,𝜎)最大，等价于对数似然函数log 𝑝(𝒚|𝑿;𝒘,𝜎)最大.令 ，得到

$$
w ^{ML} = (XX^{T}) ^{-1} X y
$$

可以看出，最大似然估计的解和最小二乘法的解相同.

## 5.4. 最大后验估计

最大似然估计的一个缺点是当训练数据比较少时会发生过拟合，估计的参数可能不准确.为了避免过拟合，我们可以给参数加上一些先验知识.假设参数𝒘为一个随机向量，并服从一个先验分布𝑝(𝒘;𝜈).为简单起见，一般令𝑝(𝒘;𝜈)为各向同性的高斯分布

$$
p(w; \nu) = \mathcal{N}(w; 0, \nu ^{2} I)
$$

其中v^2为每一维上的方差.根据贝叶斯公式，参数𝒘的后验分布(Posterior Distribution)为

$$
\begin{aligned}
    p(w|y, X; \nu, \sigma) &= \frac{p(w, y|X; \nu, \sigma)}{\sum _{w} p(w, y|X; \nu, \sigma)} \\
    & \propto p(y|X; w, \sigma) p(w; \nu)
\end{aligned}
$$

其中分母为和w无关的常数，p(y|X,w;σ)为w的似然函数，p(w;v)为先验. 这种估计参数𝒘的后验概率分布的方法称为贝叶斯估计(Bayesian Estimation)，是一种统计推断问题.采用贝叶斯估计的线性回归也称为贝叶斯线性回归(Bayesian Linear Regression).贝叶斯估计是一种参数的区间估计，即参数在一个区间上的分布.如果我们希望得到一个最优的参数值(即点估计)，可以使用最大后验估计.最大后 验估计(Maximum A Posteriori Estimation，MAP)是指最优参数为后验分布𝑝(𝒘|𝑿,𝒚;𝜈,𝜎)中概率密度最高的参数:

$$
w ^{MAP} = \arg \max _{w} p(y|X, w; \sigma)p(w; \nu)
$$

令似然函数𝑝(𝒚|𝑿,𝒘;𝜎)为高斯密度函数，则后验分布𝑝(𝒘|𝑿,𝒚;𝜈,𝜎)的对数为

$$
\begin{aligned}
    \log p(w|X, y; \nu, \sigma) & \propto \log p(y|X, w; \sigma) + \log p(w; \nu) \\
    & \propto - \frac{1}{2 \sigma ^{2}} \sum _{n = 1} ^{N} (y ^{(n)} - w^{T}x^{(n)}) ^{2} - \frac{1}{2\nu^{2}} w ^{T}w \\
    &= - \frac{1}{2 \sigma ^{2}} || y - X ^{T}w || ^{2} - \frac{1}{2\nu^{2}} w^{T}w
\end{aligned}
$$

可以看出，最大后验概率等价于平方损失的结构风险最小化，其中正则化系数𝜆=𝜎2/𝜈2.

## 5.5. 参数学习总结

|  | 无先验 | 引入先验 |
| :---: | :---: | :---: |
| 平方误差 | 经验风险最小化 | 结构风险最小化 |
| 概率 | 最大似然估计 | 最大后验估计 |
|  | $w^{ML} = (XX^{T})^{-1}Xy$ | $w ^{*} = (XX^{T} + \lambda I)^{-1}Xy$ |

> 最大后验和最大似然之间的最大区别就在于: 最大后验引入了先验概率，而最大似然没有引入先验概率.
>
> 在机器学习中，最大似然估计（MLE）和最大后验估计（MAP）是两种常用的参数估计方法。它们在应用、效果和差异方面有各自的特点：
>
> **最大似然估计（MLE）**:
>
> - **应用**: MLE是频率学派的核心方法，它不考虑参数的先验分布，只依赖于观测数据。
> - **效果**: 当数据量足够大时，MLE能提供一致且有效的参数估计。但在数据量较少时，可能会导致过拟合。
> - **特点**: MLE通过最大化观测数据的似然函数来估计参数，即寻找能使数据出现概率最大的参数值。
>
> **最大后验估计（MAP）**:
>
> - **应用**: MAP是贝叶斯学派的方法，它结合了先验知识和观测数据来估计参数。
> - **效果**: MAP在数据量较少时表现更好，因为先验知识可以起到正则化的作用，减少过拟合的风险。
> - **特点**: MAP通过最大化后验概率来估计参数，这相当于在MLE的基础上加入了与先验分布相关的惩罚项。
>
> **差异**:
>
> - **先验知识**: MLE不考虑先验知识，而MAP将先验知识作为估计的一部分。
> - **正则化效果**: MAP的先验可以看作是对参数的正则化，有助于防止模型过拟合。
> - **计算复杂性**: MAP的计算通常比MLE更复杂，因为它涉及到先验分布的计算。
>
> 总的来说，MLE适用于数据量大且没有明确先验知识的情况，而MAP适用于数据量较小或者有一定先验知识的情况。在实际应用中，选择哪种方法取决于具体问题的需求和可用数据的量
