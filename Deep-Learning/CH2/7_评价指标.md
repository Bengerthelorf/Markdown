# 9. 评价指标

为了衡量一个机器学习模型的好坏，需要给定一个测试集，用模型对测试集中的每一个样本进行预测，并根据预测结果计算评价分数.对于分类问题，常见的评价标准有准确率、精确率、召回率和F值等.给定测试集𝒯={(x^((1)),y^((1))),⋯,( x^((N)),y^((N)))}，假设标签y^((n))∈{1,⋯,𝐶}.用学习好的模型预测每个样本结果为{y ̂^((1) ),…,y ̂^((N) )}.

- 准确率:最常用的评价指标为准确率(Accuracy)，I为示性函数

$$
\mathcal{A} = \frac{1}{N} \sum _{n=1} ^{N} I(y^{(n)} = \hat{y}^{(n)})
$$

- 错误率:和准确率相对应的就是错误率(Error Rate)

$$
\begin{aligned}
    \varepsilon &= 1 - \mathcal{A} \\
    &= \frac{1}{N} \sum _{n=1} ^{N} I(y^{(n)} \neq \hat{y}^{(n)})
\end{aligned}
$$

- 精确率和召回率:准确率是所有类别整体性能的平均，如果希望对每个类都进行性能估计，就需要计算精确率(Precision)和召回率(Recall).精确率和召回率是广泛用于信息检索和统计学分类领域的两个度量值，在机器学习的评价中也被大量使用.对于类别𝑐来说，模型在测试集上的结果可以分为以下四种情况
- - 真正例(True Positive，TP):一个样本的真实类别为 𝑐 并且模型正确地预测为类别𝑐.这类样本数量记为

$$
TP_{c} = \sum _{n=1} ^{N} I(y ^{(n)} = \hat{y} ^{(n)} = c)
$$

- - 假负例(False Negative，FN):一个样本的真实类别为𝑐，模型错误地预测为其他类.这类样本数量记为

$$
FN _{c} =  \sum _{n=1} ^{N} I(y ^{(n)} \neq x \wedge \hat{y} ^{(n)} = c)
$$

- - 真负例(True Negative，TN):一个样本的真实类别为其他类，模型也预测为其他类.对于类别𝑐来说，这种情况一般不需要关注.

上述四种情况可以构筑混淆矩阵如下

|  |  | 预测类别 | 预测类别 |
| :---: | :---: | :---: | :---: |
|  |  | $\hat{y} = c$ | $\hat{y} \neq c$ |
| 真实类别 | y = c | $TP _{c}$ | $FN _{c}$ |
| 真是类别 | $y \neq c$ | $FP _{c}$ | $TN_{c}$ |

- - 精确率(Precision)，也叫精度或查准率，类别𝑐的查准率是所有预测为类别𝑐的样本中预测正确的比例:

$$
\mathcal{P} _{c} = \frac{TP _{c}}{TP _{c} + FP _{c}}
$$

- - 召回率(Recall)，也叫查全率，类别𝑐的查全率是所有真实标签为类别𝑐的样本中预测正确的比例:

$$
\mathcal{R} _{c} = \frac{TP _{c}}{TP _{c} + FN _{c}}
$$

- - F值(F-Measure)是一个综合指标，为精确率和召回率的调和平均:

$$
\mathcal{F} _{c} = \frac{(1 + \beta) ^{2} \times \mathcal{P} _{c} \times \mathcal{R} _{c}}{\beta ^{2} \times \mathcal{P} _{c} + \mathcal{R} _{c}}
$$

其中𝛽用于平衡精确率和召回率的重要性，一般取值为1.𝛽=1时的F值称为F1值，是精确率和召回率的调和平均.

- 宏平均和微平均:为了计算分类算法在所有类别上的总体精确率、召回率和F1值，经常使用两种平均方法，分别称为宏平均(Macro Average)和微平均(Micro Average).宏平均是每一类的性能指标的算术平均值:

$$
\begin{aligned}
    \mathcal{P} _{macro} &= \frac{1}{C} \sum _{c=1} ^{C} \mathcal{P} _{c} \\
    \mathcal{R} _{macro} &= \frac{1}{C} \sum _{c=1} ^{C} \mathcal{R} _{c} \\
    \mathcal{F}l _{macro} &= \frac{2 \times \mathcal{P} _{macro} \times \mathcal{R} _{macro}}{\mathcal{P} _{macro} + \mathcal{R} _{macro}}
\end{aligned}
$$

- 交叉验证(Cross-Validation)：是一种比较好的衡量机器学习模型的统计分析方法，可以有效避免划分训练集和测试集时的随机性对评价结果造成的影响.我们可以把原始数据集平均分为𝐾组不重复的子集，每次选𝐾−1组子集作为训练集，剩下的一组子集作为验证集.这样可以进行𝐾次试验并得到𝐾个模型，将这𝐾个模型在各自验证集上的错误率的平均作为分类器的评价.
