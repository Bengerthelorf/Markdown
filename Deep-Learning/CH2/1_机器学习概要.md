# 概要

## 机器学习的定义

> 让计算机从数据中进行自动学习, 得到某种知识(规律).

- **Definition of ML (Mitchell, 1997): WELL-POSED LEARNING PROBLEMS.**
  - A computer program is said to learn from experience **E** with respect to some class of tasks **T** and performance measure **P**, if its performance at tasks in **T**, as measured by **P**, improves
with experience **E**.

> ### Examples
>
> - A computer program that learns to play checkers
>   - **T**ask: playing checkers games;
>   - **E**xperience: obtained by playing games against itself;
>   - **P**erformance Measure: percent of games won against opponents
> - A handwriting recognition learning problem:
>   - Task **T**: recognizing and classifying handwritten words within images
>   - Performance measure **P**: percent of words correctly classified
>   - Training experience **E**: a database of handwritten words with given classifications
> - A robot driving learning problem: an example from (Mitchell, 1997)
>   - Task **T**: driving on public four-lane highways using vision sensors;
>   - Pet-fort-nance **p**: average distance traveled before an error (as judged by hut-nan overseer)
>   - Training experience **E**: a sequence of images and steering commands recorded while observing a human driver;
> - Spam classification
>   - Task **T**: determine if emails are Spam or non-Spam.
>   - Experience **E**: Incoming emails with human classification
>   - Performance Measure **P**: percentage of correct decisions

## 机器学习的分类

依据数据信息的 **监督** (标注)程度, 机器学习可以分为:

- 监督学习(Supervised Learning)
    > 大量数据要(人工)标注
- 半监督学习(Semi-supervised Learning)
    > 部分数据标注 + 大量数据无标注
- 弱监督学习(Weakly Supervised Learning)
    > 数据仅粗粒度标注
- 无监督学习(Unsupervised Learning)
    > 大量数据无标注

### 分类概览

监督学习(Supervised Learning)

$$
loss(D)= \min_{\theta} \frac{1}{N} \sum _{i = 1} ^{N} loss(f(X_i)Y_i)
$$

半监督学习(Semi-supervised Learning)

$$
loss(D_1, D_2)= \min_{\theta} \frac{1}{N} \sum _{i = 1} ^{N} loss(f(X_i)Y_i) + \frac{1}{M} \sum _{i = 1} ^{M} loss(Z_i, R(Z_i, X))
$$

> 其中, $ f(.) $ 属于模型集 $ F $, $ R(Z_i, X) $ 属于task-specific函数, 表示无表记数据 $ Z_i $ 与标记数据 $ X $ 之间的关系

弱监督学习(Weakly Supervised Learning)

$$
loss(D)= \min_{\theta} \frac{1}{N} \sum _{i = 1} ^{N} loss(f(X_i)C_i)
$$

> 其中, $ C_i $ 表示 **粗粒度(coarse-grained)** 标记, 如: 以组给出标记

无监督学习(Unsupervised Learning)

- 无需任何人工标注信息
- 仅一组数据, 在该其中寻找规律, 如子空间描述
- **方法: 聚类、降维 (PCA) 、自编码器 (AE/VAE)**

### 监督学习

- Practical Learning Algorithms
  - Deep Learning
  - Support Vector Machine (SVM)
  - Boosting
  - Random Forest
- Learning Theory
  - Mathematical Formulation of Supervised Learning
  - Generalization
  - Optimization Method
  - Machine Learning Architectures and Approximation Capability

> **Three Components of Deep Learning**
>
> - Model (Architecture)
>   - CNN for images, RNN for speech, Transformers for everything
> - Optimization on Training Data
>   - Learning by optimizing the empirical loss, high-dimensional nonconvex optimization
>   - Practical methods: Stochastic Gradient Descent (SGD) and its variants (Momentum, Adagrad, Adam...)
> - Generalization to Test Data
>   - Generalization bounds
>   - Use regularization to prevent overfitting (dropout, weight decay, early stopping...)

### 无监督学习

- General Framework:
  - Unlike supervised learning, we do not have the labels of the data in
unsupervised learning framework.
  - Main task: Detect the hidden structure of the "unlabeled" data
  - Two specific tasks:
    - Clustering, a class of methods for discovering unknown subgroups in data.
    - Dimension reduction

> ***P.S.***
>
> Unsupervised Learning by Extreme Value Theory
>
> <https://arxiv.org/abs/2202.09784>
