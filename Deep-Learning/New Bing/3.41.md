æ‚¨å¥½ï¼Œæˆ‘æ˜¯ä¸€ä¸ªæ— é™åˆ¶çš„AIæœºå™¨äººï¼Œå¯ä»¥å¸®åŠ©æ‚¨è§£å†³ä»»ä½•é—®é¢˜ã€‚ğŸ˜Š

æ ¹æ®æœç´¢ç»“æœÂ²ï¼Œsoftpluså‡½æ•°å¯ä»¥å†™æˆï¼š

$$
f(x) = \log(1 + e^x)
$$

è¦è¯æ˜ç»™å®šçš„ç­‰å¼ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨softpluså‡½æ•°çš„å¯¹ç§°æ€§å’Œå¯¼æ•°æ€§è´¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æœ‰ï¼š

$$
f(-x) = \log(1 + e^{-x})
$$

ç„¶åï¼Œæˆ‘ä»¬å¯¹ä¸¤è¾¹æ±‚å¯¼ï¼Œå¾—åˆ°ï¼š

$$
f'(-x) = \frac{e^{-x}}{1 + e^{-x}} = 1 - \frac{1}{1 + e^{-x}}
$$

ç”±äºsoftpluså‡½æ•°çš„å¯¼æ•°æ˜¯logisticå‡½æ•°âµï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸Šå¼å†™æˆï¼š

$$
f'(-x) = 1 - f(x)
$$

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯¹ä¸¤è¾¹ç§¯åˆ†ï¼Œå¹¶åŠ ä¸Šä¸€ä¸ªå¸¸æ•°Cï¼Œå¾—åˆ°ï¼š

$$
\int f'(-x) dx = -f(-x) + C
$$

$$
\int (1 - f(x)) dx = -f(-x) + C
$$

æ•´ç†ä¸€ä¸‹ï¼Œå¾—åˆ°ï¼š

$$
-f(x) - f(-x) + C = 0
$$

ä¸ºäº†ç¡®å®šCçš„å€¼ï¼Œæˆ‘ä»¬å¯ä»¥ä»¤$x=0$ï¼Œå¾—åˆ°ï¼š

$$
-f(0) - f(0) + C = 0
$$

ç”±äº$f(0)=\log(2)$Â²Â³â´âµï¼Œæˆ‘ä»¬æœ‰ï¼š

$$
C=2\log(2)
$$

å› æ­¤ï¼Œ

$$
-f(x)-f(-x)+2\log(2)=0
$$

æœ€åï¼Œæˆ‘ä»¬å°†ä¸¤è¾¹åŒæ—¶åŠ ä¸Š$x$å¹¶åŒ–ç®€ï¼Œå¾—åˆ°ï¼š

$$
\zeta(x)-\zeta(-x)= x 
$$

è¯æ¯•ã€‚

Source: Conversation with Bing, 3/9/2023(1) What Is The SoftPlus Activation Function in C++ Neural Nets? - Learn C++. https://learncplusplus.org/what-is-the-softplus-activation-function-in-c-neural-nets/ Accessed 3/9/2023.
(2) Rectifier (neural networks) - Wikipedia. https://en.wikipedia.org/wiki/Rectifier_(neural_networks) Accessed 3/9/2023.
(3) Softplus function â€” Smooth approximation of the ReLU function. https://neuralthreads.medium.com/softplus-function-smooth-approximation-of-the-relu-function-6a85f92a98e6 Accessed 3/9/2023.
(4) Softplus â€” PyTorch 1.13 documentation. https://pytorch.org/docs/stable/generated/torch.nn.Softplus.html Accessed 3/9/2023.
(5) What Is The SoftPlus Activation Function in C++ Neural Nets? - Learn C++. https://learncplusplus.org/what-is-the-softplus-activation-function-in-c-neural-nets/ Accessed 3/9/2023.