# Outline of the course

> è¯¾æ—¶18å‘¨
>
> è€ƒè¯•å†…å®¹æˆªè‡³åˆ°13thè¯¾, å¦‚ä¹‹åæœ‰å¢åŠ å†…å®¹ä¼šå†æåŠ

- [Outline of the course](#outline-of-the-course)
  - [è¯¾æ¬¡å…·ä½“å†…å®¹](#è¯¾æ¬¡å…·ä½“å†…å®¹)
    - [Week 1](#week-1)
    - [Week 2: æ•°å­¦æ¦‚å¿µ](#week-2-æ•°å­¦æ¦‚å¿µ)
    - [Week 3: æ¯ä¸‰å‘¨è®²ä¸€æ¬¡ç¼–ç¨‹ç›¸å…³](#week-3-æ¯ä¸‰å‘¨è®²ä¸€æ¬¡ç¼–ç¨‹ç›¸å…³)
    - [Week 4: å·ç§¯, BPç®—æ³•](#week-4-å·ç§¯-bpç®—æ³•)
    - [Week 5](#week-5)
    - [Week 6: è®­ç»ƒç¥ç»ç½‘ç»œçš„å®è·µ](#week-6-è®­ç»ƒç¥ç»ç½‘ç»œçš„å®è·µ)
    - [Week 7: ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–](#week-7-ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–)
    - [Week 8: ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–](#week-8-ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–)
    - [Week 9: ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–](#week-9-ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–)
    - [Week 10: RNN](#week-10-rnn)
    - [Week 11](#week-11)
    - [Week 12](#week-12)
    - [Week 13](#week-13)
    - [Week 14: å¾…å®š](#week-14-å¾…å®š)
    - [Week 15: å¾…å®š](#week-15-å¾…å®š)
    - [Week 16](#week-16)

## è¯¾æ¬¡å…·ä½“å†…å®¹

### Week 1

1. Introduction to Neural Network (Chap 1 of book) ç¥ç»ç½‘ç»œç®€ä»‹
   1. The history and recent development of Neural Network (NN) and Deep Neural Network (DNN);  ç¥ç»ç½‘ç»œå†å²ç®€ä»‹
   2. Some basic concepts and examples in NN & DNN; åŸºæœ¬æ¦‚å¿µç®€ä»‹
   3. the dataset (mnist, cifar-10, cifar-100) used in DNN; æ•°æ®é›†ç®€ä»‹
   4. An introduction to the coding toolbox of NN & DNN; ç¥ç»ç½‘ç»œå·¥å…·åŒ…ç®€ä»‹

### Week 2: æ•°å­¦æ¦‚å¿µ

1. Mathematics and Machine learning Basic: (Chap. 2, 3 and 5 of the book) æ•°å­¦åŠæœºå™¨å­¦ä¹ å†…å®¹å›é¡¾
   1. A Recap of Math and Programming; æ•°å­¦åŠç¼–ç¨‹å›é¡¾
   2. Overview and key concepts of Machine learning and Statistical learning (overfitting) é‡ç‚¹ç»Ÿè®¡æœºå™¨å­¦ä¹ æ¦‚å¿µå›é¡¾(å¦‚è¿‡æ‹Ÿåˆ)
   3. The motivation of developing DNN. ç ”ç©¶ç¥ç»ç½‘ç»œçš„åŠ¨æœº
   4. "Classical" Neural Network â€“ MLP  ç»å…¸ç¥ç»ç½‘ç»œ
      - Multi-layer perception;  å¤šå±‚æ„ŸçŸ¥æœº
      - Radial basis function and Radial basis function network; å¾„å‘åŸºç½‘ç»œç­‰
      - Other types of Neural Network (e.g. Self-organizing map) è‡ªç»„ç»‡ç¥ç»ç½‘ç»œ

### Week 3: æ¯ä¸‰å‘¨è®²ä¸€æ¬¡ç¼–ç¨‹ç›¸å…³

Tutorial about Pytorch, Tensorflow, and Mindspore  Portchã€TensorflowåŠMindsporeæ•™ç¨‹

### Week 4: å·ç§¯, BPç®—æ³•

1. Convolutional Neural Network (CNN): (Chap6, Chap 9 of the book) å·ç§¯ç¥ç»ç½‘ç»œ
   1. Introduction to CNN  CNNç®€ä»‹
   2. Convolutional layer, pooling, ReLU,  CNNå‡ ä¸ªåŸºæœ¬æ„ä»¶
   3. Different CNN structures for image classification  å›¾åƒåˆ†ç±»çš„å‡ ä¸ªCNN
   4. BP algortihm BPç®—æ³•

### Week 5

1. Regularization of CNN&MLP (Chap 7 of the book) ç¥ç»ç½‘ç»œçš„æ­£åˆ™åŒ–
   1. Weight initialization  æƒé‡åˆå§‹åŒ–
   2. L1, L2 regularization  L1ã€L2æ­£åˆ™è¯
   3. Data augmentation, Early stopping, Dropout  æ•°æ®å¢å¹¿ã€æ—©åœã€Dropoutç­‰æœºåˆ¶

> æ­£åˆ™åŒ–

### Week 6: è®­ç»ƒç¥ç»ç½‘ç»œçš„å®è·µ

1. Babysitting the training of CNN  è®­ç»ƒç¥ç»ç½‘ç»œå®è·µ
   1. Training CNN on cifar10, MNIST åœ¨cifar10ç­‰æ•°æ®é›†è®­ç»ƒ
   2. A tutorial of Mixup  Mixupç­–ç•¥
2. Lab: Practical coding issues: SGD, Data augmentation, Batch Normalization;  å®éªŒï¼šå®ç”¨ç¼–ç¨‹æŠ€å·§ã€SGDã€æ•°æ®å¢å¹¿ã€æ‰¹æ­£åˆ™åŒ–

### Week 7: ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–

1. Optimization for Training Deep Modelsâ€”Part 1 (Chap 4 and Chap 8.1â€”8.2 of the book) ç¥ç»ç½‘ç»œä¼˜åŒ–(1)
   1. Numerical computing basics  æ•°å€¼è®¡ç®—é‡åˆ°çš„é—®é¢˜
   2. Optimization Basics  ä¼˜åŒ–åŸºç¡€
   3. The challenges in Optimization deep neural networks;  è®­ç»ƒç¥ç»ç½‘è·¯çš„æŒ‘æˆ˜

> 789 ç¥ç»ç½‘ç»œçš„ä¼˜åŒ– ğŸ‘‰ ç”±æ­¤éœ€è¦æ•°å­¦çš„åŸºç¡€

### Week 8: ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–

1. Optimization for Training Deep Modelsâ€”Part 2 (Chap 8.3 â€“8.7 of the book) ç¥ç»ç½‘ç»œä¼˜åŒ–(2)
   1. SGD: motivation, implementation, variants, Pros/Cons.  SGDï¼šåŠ¨æœºã€å®ç°ã€è¡ç”Ÿæ–¹æ³•ï¼ŒåŠä¼˜ç¼ºç‚¹
   2. AdaGrad, RMSProp, Nesterov, Adam: motivation, implementation, variants, Pros/Cons.  AdaGrad, RMSPropç­‰æ–¹æ³•ç®€ä»‹
   3. Recent Deeplearning Optimizer: ***Lars***, Lamb, DessiLBI  æœ€æ–°çš„ç¥ç»ç½‘ç»œä¼˜åŒ–å™¨

> 789 ç¥ç»ç½‘ç»œçš„ä¼˜åŒ– ğŸ‘‰ ç”±æ­¤éœ€è¦æ•°å­¦çš„åŸºç¡€
> Lars é‡ç‚¹

### Week 9: ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–

1. Optimization Tutorial (Pytorch)  (Chap 11 of the book)  ä¼˜åŒ–çš„æ•™ç¨‹
   1. How to write an optimizer for Pytorch?  å¦‚ä½•å†™ä¸€ä¸ªPytorchä¼˜åŒ–å™¨
   2. Optimizer code reading: SGD, Nesterov, Adam  ä¼˜åŒ–ä»£ç è§£è¯»
   3. Batch Normalization helps Optimization, (Loss landscape)  ä¸ºä»€ä¹ˆæ‰¹æ­£åˆ™åŒ–ä¼šå¸®åŠ©ä¼˜åŒ–

> 789 ç¥ç»ç½‘ç»œçš„ä¼˜åŒ– ğŸ‘‰ ç”±æ­¤éœ€è¦æ•°å­¦çš„åŸºç¡€

### Week 10: RNN

1. Recurrent Neural Network(RNN) (Chap 9 of the book) å¾ªç¯ç¥ç»ç½‘ç»œ
   1. Vanilla methods in modelling Sequential Data: N-gram, HMM, word2vec  å¤„ç†åºåˆ—æ•°æ®çš„ä¸€èˆ¬æ–¹æ³•
   2. Recurrent Neural Networks; å¾ªç¯ç¥ç»ç½‘ç»œ
   3. Variants of RNN, and examples of RNNs  å¾ªç¯ç¥ç»ç½‘ç»œçš„è¡ç”Ÿ

### Week 11

1. Long short-term memory (LSTM) and Auto-Encoder (Chap 13, 14 of the book)  LSTMç½‘ç»œåŠè‡ªç¼–ç å™¨ç½‘ç»œ
   1. LSTM models and its variants  LSTMç½‘ç»œåŠå˜ç§
   2. Auto-encoder, U-Net and its variants è‡ªç¼–ç å™¨åŠå˜ç§
   3. Transformers  å˜å½¢å™¨ç½‘ç»œ
   4. Applications of LSTM, and Autoencoders  æ¨¡å‹çš„åº”ç”¨

### Week 12

1. Lab and Mid-term Review, (Chap 12 of the book)  å®éªŒåŠä¸­æœŸå›é¡¾
   1. Works on object detections  ç‰©ä½“æ£€æµ‹
   2. Learning sequential models  åºåˆ—åŒ–æ¨¡å‹å­¦ä¹ 
   3. OCR examples: text detection, and text recognition  OCRä¾‹å­ç­‰

### Week 13

1. GANs  å¯¹æŠ—ç”Ÿæˆç½‘ç»œ
   1. Basics of GANs  GANçš„åŸºç¡€
   2. Variants of GANs: DC-GAN, SN-GAN, StyleGAN    GANçš„è¡ç”Ÿæ¨¡å‹
   3. Image Translation and syntheze Examples: CycleGAN, StackGAN   åŸºäºGANçš„å›¾åƒå¤„ç†

### Week 14: å¾…å®š

1. Other Advanced Topics: é«˜çº§è¯é¢˜
   1. Meta-learning and one-shot learning  å…ƒå­¦ä¹ åŠå°æ ·æœ¬å­¦ä¹ 
   2. Learning based 3D reconstruction   åŸºäºå­¦ä¹ çš„ä¸‰ç»´é‡å»º

### Week 15: å¾…å®š

1. Lab: Advanced Topics:  å®éªŒï¼šé«˜çº§é¢˜ç›®
   1. examples of one-shot learning  å°æ ·æœ¬å­¦ä¹ ä¾‹å­
   2. examples of meta-learning   å…ƒå­¦ä¹ çš„ä¾‹å­
   3. Examples of learning based 3D reconstruction   åŸºäºå­¦ä¹ çš„ä¸‰ç»´é‡å»ºä¾‹å­

### Week 16

Summary and Review  æ€»ç»“åŠè¯¾ç¨‹å›é¡¾
