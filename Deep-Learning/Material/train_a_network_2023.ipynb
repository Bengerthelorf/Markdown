{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "c4I7EfafVI_h"
   },
   "source": [
    "# Step-by-step Guide to Train a Network\n",
    "## 0. Summary\n",
    "This tutorial includes the following three parts:\n",
    "\n",
    "1. Train a LeNet-5 on MNIST;\n",
    "\n",
    "2. Train a VGG and a ResNet on CIFAR-10;\n",
    "\n",
    "3. Train a network using mixup strategy.\n",
    "\n",
    "This tutorial is based on the [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgZR_LqFVI_y"
   },
   "source": [
    "## 1. Preparation\n",
    "\n",
    "We will do the following steps to train a network:\n",
    "\n",
    "1. Load and normalizing the dataset;\n",
    "\n",
    "2. Define the network;\n",
    "\n",
    "3. Decide to use which loss function, optimizer and/or other stategies;\n",
    "\n",
    "4. Train the network on the training data;\n",
    "\n",
    "5. Test the network on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yiEMvyjqVI_z"
   },
   "outputs": [],
   "source": [
    "import errno\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.models.vgg import vgg11\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    # for windows and linux with GPU support\n",
    "    device = 'cuda:0'\n",
    "elif torch.backends.mps.is_available():\n",
    "    # for Apple silicon Mac\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eWX6LM1VI_0"
   },
   "source": [
    "### 1.1 Load and normalizing the dataset\n",
    "In this lecture, we will use two datasets, MNIST and CIFAR-10. In our experiments, we will use *torchvision* to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bF9o1Vp2VI_0"
   },
   "outputs": [],
   "source": [
    "mnist_data_path = 'data/mnist'\n",
    "mnist_transform = T.Compose([\n",
    "    T.Resize((32, 32)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "mnist_train = MNIST(mnist_data_path, download=True, transform=mnist_transform)\n",
    "mnist_test = MNIST(mnist_data_path, train=False, transform=mnist_transform)\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=1024, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9acbxCZPVI_2",
    "outputId": "a88d4d7d-7a3e-424c-97c7-07d2d16203bd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_data_path = 'data/cifar10'\n",
    "normalize = T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "cifar_train_transform = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "cifar_test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "cifar_train = CIFAR10(cifar_data_path, download=True, transform=cifar_train_transform)\n",
    "cifar_test = CIFAR10(cifar_data_path, train=False, transform=cifar_test_transform)\n",
    "cifar_train_loader = DataLoader(cifar_train, batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n",
    "cifar_test_loader = DataLoader(cifar_test, batch_size=1024, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xZPqps6VI_3"
   },
   "source": [
    "### 1.2 Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WRPf__ZCVI_3"
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.extractor = nn.Sequential(OrderedDict([\n",
    "            ('c1', nn.Conv2d(1, 6, kernel_size=(5, 5))),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('s2', nn.MaxPool2d(kernel_size=(2, 2), stride=2)),\n",
    "            ('c3', nn.Conv2d(6, 16, kernel_size=(5, 5))),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('s4', nn.MaxPool2d(kernel_size=(2, 2), stride=2)),\n",
    "            ('c5', nn.Conv2d(16, 120, kernel_size=(5, 5))),\n",
    "            ('relu3', nn.ReLU()),\n",
    "        ]))\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "            ('f6', nn.Linear(120, 84)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('f7', nn.Linear(84, num_classes)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.extractor(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "lenet = LeNet5(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ydDZPaMlVI_4"
   },
   "outputs": [],
   "source": [
    "vgg = vgg11(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hEk21F8dVI_5"
   },
   "outputs": [],
   "source": [
    "'''ResNet in PyTorch.\n",
    "BasicBlock and Bottleneck module is from the original ResNet paper:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "PreActBlock and PreActBottleneck module is from the later paper:\n",
    "[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = conv3x3(3,64)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, lin=0, lout=5):\n",
    "        out = x\n",
    "        if lin < 1 and lout > -1:\n",
    "            out = self.conv1(out)\n",
    "            out = self.bn1(out)\n",
    "            out = F.relu(out)\n",
    "        if lin < 2 and lout > 0:\n",
    "            out = self.layer1(out)\n",
    "        if lin < 3 and lout > 1:\n",
    "            out = self.layer2(out)\n",
    "        if lin < 4 and lout > 2:\n",
    "            out = self.layer3(out)\n",
    "        if lin < 5 and lout > 3:\n",
    "            out = self.layer4(out)\n",
    "        if lout > 4:\n",
    "            out = F.avg_pool2d(out, 4)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(PreActBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pq3XgSUVI_5"
   },
   "source": [
    "### 1.3 Decide to use which loss function\n",
    "In our experiments, we will use the Cross-Entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SJ92OEULVI_6"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hora9W6HVI_8"
   },
   "source": [
    "### 1.4 Define the training and test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fCC54_ICVI_8"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def mkdir_if_missing(directory):\n",
    "    if not osp.exists(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "def save_checkpoint(state, is_best=False, fpath=''):\n",
    "    if len(osp.dirname(fpath)) != 0:\n",
    "        mkdir_if_missing(osp.dirname(fpath))\n",
    "    torch.save(state, fpath)\n",
    "    if is_best:\n",
    "        shutil.copy(fpath, osp.join(osp.dirname(fpath), 'best_model.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, initial_lr):\n",
    "    \"\"\"decrease the learning rate at 100 and 150 epoch\"\"\"\n",
    "    lr = initial_lr\n",
    "    if epoch >= 100:\n",
    "        lr /= 10\n",
    "    if epoch >= 150:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "def warp_tqdm(data_loader, disable_tqdm):\n",
    "    if disable_tqdm:\n",
    "        tqdm_loader = data_loader\n",
    "    else:\n",
    "        tqdm_loader = tqdm(data_loader, ncols=0)\n",
    "    return tqdm_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cNhQ8Fm4VI_9"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for input, target in warp_tqdm(train_loader, True):\n",
    "\n",
    "\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "\n",
    "    log = 'Epoch:{:03} Time:{:.3f}s Loss: {loss.avg:.4f} '.format(epoch, time.time()-start_time, loss=losses)\n",
    "    return log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T0-f-0ELVI__"
   },
   "outputs": [],
   "source": [
    "def test(test_loader, model, criterion):\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for input, target in test_loader:\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(input)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1 = accuracy(output.data, target)[0]\n",
    "        top1.update(acc1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "\n",
    "    log = 'Test Acc@1: {top1.avg:.3f}'.format(top1=top1)\n",
    "\n",
    "    return top1.avg, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AglgnEvkVJAA"
   },
   "source": [
    "## 2. Train a LeNet-5 on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLIWwz41VJAB",
    "outputId": "a5324a6a-9597-4539-a5d7-dc63d713cfd8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:000 Time:3.582s Loss: 2.3022 Test Acc@1: 12.190\n",
      "Epoch:001 Time:2.997s Loss: 2.2976 Test Acc@1: 18.230\n",
      "Epoch:002 Time:2.850s Loss: 2.2912 Test Acc@1: 33.600\n",
      "Epoch:003 Time:2.990s Loss: 2.2777 Test Acc@1: 37.680\n",
      "Epoch:004 Time:2.945s Loss: 2.2210 Test Acc@1: 47.120\n",
      "Epoch:005 Time:2.966s Loss: 1.5398 Test Acc@1: 79.520\n",
      "Epoch:006 Time:2.853s Loss: 0.5888 Test Acc@1: 88.070\n",
      "Epoch:007 Time:2.860s Loss: 0.4033 Test Acc@1: 89.920\n",
      "Epoch:008 Time:2.989s Loss: 0.3297 Test Acc@1: 91.870\n",
      "Epoch:009 Time:2.857s Loss: 0.2816 Test Acc@1: 92.800\n",
      "Epoch:010 Time:2.923s Loss: 0.2458 Test Acc@1: 93.670\n",
      "Epoch:011 Time:2.913s Loss: 0.2188 Test Acc@1: 94.120\n",
      "Epoch:012 Time:2.954s Loss: 0.1972 Test Acc@1: 94.780\n",
      "Epoch:013 Time:2.881s Loss: 0.1795 Test Acc@1: 95.490\n",
      "Epoch:014 Time:2.962s Loss: 0.1651 Test Acc@1: 95.530\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = 'experiments/mnist'\n",
    "num_epochs = 15\n",
    "model = lenet.to(device)\n",
    "train_loader = mnist_train_loader\n",
    "test_loader = mnist_test_loader\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train_log = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    acc, test_log = test(test_loader, model, criterion)\n",
    "    log = train_log + test_log\n",
    "    print(log)\n",
    "    is_best = acc > best_acc\n",
    "    best_acc = max(acc, best_acc)\n",
    "    if is_best:\n",
    "        save_checkpoint({'epoch':epoch,\n",
    "        'state_dict':model.state_dict(),\n",
    "        'acc': acc,\n",
    "        }, False,  os.path.join(ckpt_dir, 'best_model.pth.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G1YXmc3VJAC"
   },
   "source": [
    "## 3. Train a VGG/ResNet on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCZWLEilVJAC",
    "outputId": "d08951b4-cda8-4d95-e35a-3cf7d25ccef9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:000 Time:84.020s Loss: 2.2953 Test Acc@1: 10.650\n",
      "Epoch:001 Time:84.599s Loss: 2.0956 Test Acc@1: 28.860\n",
      "Epoch:002 Time:84.586s Loss: 1.7746 Test Acc@1: 41.350\n",
      "Epoch:003 Time:85.737s Loss: 1.5418 Test Acc@1: 45.290\n",
      "Epoch:004 Time:84.972s Loss: 1.4598 Test Acc@1: 49.550\n",
      "Epoch:005 Time:85.266s Loss: 1.4081 Test Acc@1: 52.110\n",
      "Epoch:006 Time:85.476s Loss: 1.2246 Test Acc@1: 55.140\n",
      "Epoch:007 Time:85.660s Loss: 1.1308 Test Acc@1: 64.620\n",
      "Epoch:008 Time:85.526s Loss: 1.0546 Test Acc@1: 68.370\n",
      "Epoch:009 Time:86.257s Loss: 0.9687 Test Acc@1: 69.850\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = 'experiments/cifar10-vgg'\n",
    "# num_epochs = 200\n",
    "num_epochs = 10\n",
    "model = vgg.to(device)\n",
    "train_loader = cifar_train_loader\n",
    "test_loader = cifar_test_loader\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, 0.1)\n",
    "    train_log = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    acc, test_log = test(test_loader, model, criterion)\n",
    "    log = train_log + test_log\n",
    "    print(log)\n",
    "    is_best = acc > best_acc\n",
    "    best_acc = max(acc, best_acc)\n",
    "    if is_best:\n",
    "        save_checkpoint({'epoch':epoch,\n",
    "        'state_dict':model.state_dict(),\n",
    "        'acc': acc,\n",
    "        }, False,  os.path.join(ckpt_dir, 'best_model.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13fEzbsUVJAD",
    "outputId": "d3717661-4419-4cf2-fd4f-68de1a0511e2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:000 Time:115.617s Loss: 1.6182 Test Acc@1: 49.540\n",
      "Epoch:001 Time:116.192s Loss: 1.0852 Test Acc@1: 62.220\n",
      "Epoch:002 Time:116.093s Loss: 0.8269 Test Acc@1: 69.960\n",
      "Epoch:003 Time:115.787s Loss: 0.6583 Test Acc@1: 76.500\n",
      "Epoch:004 Time:115.703s Loss: 0.5580 Test Acc@1: 80.520\n",
      "Epoch:005 Time:116.433s Loss: 0.4834 Test Acc@1: 77.030\n",
      "Epoch:006 Time:117.487s Loss: 0.4288 Test Acc@1: 82.980\n",
      "Epoch:007 Time:118.290s Loss: 0.3894 Test Acc@1: 80.960\n",
      "Epoch:008 Time:117.545s Loss: 0.3476 Test Acc@1: 85.240\n",
      "Epoch:009 Time:117.586s Loss: 0.3209 Test Acc@1: 85.870\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = 'experiments/cifar10-resnet'\n",
    "# num_epochs = 200\n",
    "num_epochs = 10\n",
    "model = ResNet18().to(device)\n",
    "train_loader = cifar_train_loader\n",
    "test_loader = cifar_test_loader\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, 0.1)\n",
    "    train_log = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    acc, test_log = test(test_loader, model, criterion)\n",
    "    log = train_log + test_log\n",
    "    print(log)\n",
    "    is_best = acc > best_acc\n",
    "    best_acc = max(acc, best_acc)\n",
    "    if is_best:\n",
    "        save_checkpoint({'epoch':epoch,\n",
    "        'state_dict':model.state_dict(),\n",
    "        'acc': acc,\n",
    "        }, False,  os.path.join(ckpt_dir, 'best_model.pth.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgB2-_P6VJAE"
   },
   "source": [
    "## 4. MixUp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MixUp is a simple strategy to alleviate the problem of *memorizaiton* and *sensitivity to adversarial examples* when training deep neural networks. Basically, network trained with MixUp uses convex combinations of data pairs(images and labels) for training. This regularizes the network to favor simple linear behavior in-between training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two data pairs $(x_i, y_i)$ and $(x_j, y_j)$, the MixUp virtual training example is constructed as \n",
    "$$\\tilde{x}=\\lambda x_i + (1-\\lambda) x_j,\\quad\\textrm{where }x_i,x_j \\textrm{ are raw input vectors}$$\n",
    "$$\\tilde{y}=\\lambda y_i + (1-\\lambda) y_j,\\quad\\textrm{where }y_i,y_j \\textrm{ are one-hot label encodings}$$\n",
    "where $\\lambda\\sim\\mathrm{Beta}(\\alpha,\\alpha),\\alpha\\in(0,\\infty)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).to(device)\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mixup(train_loader, model, criterion, optimizer, alpha, epoch):\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for input, target in warp_tqdm(train_loader, True):\n",
    "\n",
    "\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        input, target_a, target_b, lam = mixup_data(input, target, alpha, True)\n",
    "        input, target_a, target_b = map(Variable, (input, target_a, target_b))\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = mixup_criterion(criterion, output, target_a, target_b, lam)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "\n",
    "    log = 'Epoch:{:03} Time:{:.3f} Loss: {loss.avg:.4f} '.format(epoch, time.time() - start_time, loss=losses)\n",
    "    return log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a ResNet on CIFAR-10 with MixUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:000 Time:119.481 Loss: 2.1024 Test Acc@1: 32.690\n",
      "Epoch:001 Time:120.603 Loss: 1.8866 Test Acc@1: 50.470\n",
      "Epoch:002 Time:119.001 Loss: 1.7622 Test Acc@1: 56.830\n",
      "Epoch:003 Time:117.712 Loss: 1.6800 Test Acc@1: 59.980\n",
      "Epoch:004 Time:117.717 Loss: 1.6073 Test Acc@1: 60.210\n",
      "Epoch:005 Time:118.680 Loss: 1.5544 Test Acc@1: 66.940\n",
      "Epoch:006 Time:117.109 Loss: 1.5052 Test Acc@1: 66.130\n",
      "Epoch:007 Time:115.651 Loss: 1.4903 Test Acc@1: 70.520\n",
      "Epoch:008 Time:115.897 Loss: 1.4607 Test Acc@1: 73.250\n",
      "Epoch:009 Time:115.572 Loss: 1.4349 Test Acc@1: 75.310\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = 'experiments/cifar10-resnet-mixup'\n",
    "alpha = 1.0\n",
    "# num_epochs = 200\n",
    "num_epochs = 10\n",
    "model =ResNet18().to(device)\n",
    "train_loader = cifar_train_loader\n",
    "test_loader = cifar_test_loader\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch, 0.1)\n",
    "    train_log = train_mixup(train_loader, model, criterion, optimizer, alpha, epoch)\n",
    "    acc, test_log = test(test_loader, model, criterion)\n",
    "    log = train_log + test_log\n",
    "    print(log)\n",
    "    is_best = acc > best_acc\n",
    "    best_acc = max(acc, best_acc)\n",
    "    if is_best:\n",
    "        save_checkpoint({'epoch':epoch,\n",
    "        'state_dict':model.state_dict(),\n",
    "        'acc': acc,\n",
    "        }, False, os.path.join(ckpt_dir, 'best_model.pth.tar'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "train_a_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "e4c6ce54e6d1ccff551279c9aafc06b78c48fd9e60d6b4e74c0583a74ec1d1f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
