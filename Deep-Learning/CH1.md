# CH1

## 1. couse information

### 1.1. Abstract

***PS. ç¬¬ä¸‰å‘¨ä¼šè®²pytorch***

Office hours: å‘¨å››å‘¨äº” 3-5.30pm

Email: <deeplearning.fudan@yandex.com>

#### Textbook

##### main

- èŠ±çš®ä¹¦ (ä¸ºä¸»)
- é‚± ä¹¦
- å¬ä¸æ‡‚, è¿‡äºåƒåœ¾çš„è‹±è¯­å£éŸ³

##### extra

![extra_textbook](./img/ch1_extra_textbook.png)

> ç¬¬äºŒæœ¬ä¸é”™(?)

---

å‰ç½®è¯¾ç¨‹ ğŸ‘‰ ç»Ÿè®¡æœºå™¨å­¦ä¹ 

- Final scorce =
  - Class attendance/discussion (10%);
  - Homework Assignment and Projects (30%); 2-3; æ¯ä¸ªâ¼ˆç‹¬â½´å®Œæˆ ğŸ‘‰ æ‰‹å†™ç®—æ³•ä¹‹ç±»çš„
  - Final Exam (60%). ğŸ‘‰ è€ƒè¯•çš„

- Courses:
  - We will give some tutorial.
- Pain and Happiness:
  - Huge efforts to code, debug, read and think;
  - Worth doing it!!

### 1.2. Conferences

- OpenAI ç½‘ç«™
  - ç½‘ç«™ä¸Špostçš„å°±ç®—æ˜¯å‘è¡¨äº† :D
- Machine Learning
  - NeurIPS
  > è¿‡å»æ˜¯ä¸»è¦æ˜¯è´å¶æ–¯çš„å†…å®¹éƒ½åœ¨è¿™é‡Œ,ä½†æ˜¯ç°åœ¨å·²ç»æ˜¯å¾ˆæ‚çš„å‡ ä¹å…¨è¦†ç›–çš„å†…å®¹
  - ICML
  > åé‡äºML, æ•°å­¦å†…å®¹ç›¸å¯¹å¤š
  - ECML
  > æ¯”ICMLä½ä¸€ä¸ªç­‰çº§,åé‡äºè´å¶æ–¯æ–¹æ³•ç­‰çš„
  - UAI
  > å°çš„, åŒ»å­¦, é€šè¿‡è´å¶æ–¯åšåŒ»å­¦çš„
  - COLT
  > æ›´åé‡æ•°å­¦
  - ASTATS
  > ç»Ÿè®¡çš„å†…å®¹, è´¨é‡ä¸é”™
- AI In general
  - AAAI
  > å¯¹äºCVä¹‹ç±»çš„è€Œè¨€: åˆ«çš„æŠ•ä¸å‡ºå»å°±æ˜¯è¿™ä¸ªæ
  - IJCAI
  > ä¹Ÿæ˜¯ç›¸å¯¹æ¯”è¾ƒæ‹‰æ‹‰æ
- Pattern Recognition & Computer Vision
  - ECCV
  > æ¯åŒæ•°å¹´ (æ¬§æ´²)
  - ICCV
  > æ¯å•æ•°å¹´ (ä¸–ç•Œ)
  - CVPR
  - ICPR/ACCV/BMVC
- Data Mining
  - ACM SIGKDD
  - ACM SIGIR/ICDM
- Natural Language Processing
  - ACL/EMNLP/COLING

### 1.3. Joural

- ML & AI
  - JMRL
  > åé‡ç†è®ºçš„
  - TPMAI
  - Artificial intelligent
  - IJCM
  > å¥½çš„, ä¸ç”¨ç‰ˆé¢è´¹çš„
- Statistics
  - The Annals of Statistics
- Pattern Recognition & NN
  - Neural Computation
  - Neural Networks
  - IEEE Transactions on Neural Networks and Learning System
- Data Mining
  - IEEE Transactions on Knowledge and Data Engineering (TKDE)

### 1.4 info website

[AI Conference Deadlines](https://aideadlin.es/?sub=ML,CV)

## 2. äººå·¥æ™ºèƒ½

- ç¬¦å·ä¸»ä¹‰æ–¹æ³•
- è”ç»“ä¸»ä¹‰æ–¹æ³•
  - è¿‘å‡ å¹´çš„æ–¹æ³•,ç¥ç»å…ƒ
- è¡Œä¸ºä¸»ä¹‰æ–¹æ³•

### 2.1. æœºå™¨å­¦ä¹ 

***ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ , æœºå™¨å­¦ä¹ çš„å®šä¹‰:***

**TEP:**

> A computer program is said to learn from experience **E** with respect to some class of tasks **T** and performance measure **P**, if its performance at tasks in **T**, as measured by **P**, improves with experience **E**.

- T ğŸ‘‰ Task
- P ğŸ‘‰ Perfomence measure
- E ğŸ‘‰ Training experience

### 2.2. ç›‘ç£å­¦ä¹ 

1. Given training data
    $$
    \{ x_{1},y_{1} \} ^{N}_{i=1}
    $$
2. Choose each of these
   - Decision Function
    $$
    \hat{y}=f _\theta (x_i)
    $$
   - loss function
    $$
    \mathscr{L}(\hat{y},y_i) \in \mathbb{R}
    $$
3. Define goal
    $$
    \theta ^* = arg \underset{\theta}{min} \sum _{i=1} ^N \mathscr{L}(f _\theta (x_i) ,y)
    $$
4. Train with SGD
    (take small steps opposite the gradient)
    $$
    \theta ^{(t+1)} = \theta ^{(t)} - \eta _t \nabla \mathscr{L}(f _\theta (x_i) ,y _i)
    $$
  
*PS.ä¸ºä»€ä¹ˆæœ‰æ—¶å€™æ¢¯åº¦æ±‚é”™äº†ç»“æœåè€Œæ›´å¥½* :D

> æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºå¯»æ‰¾ä¸€ä¸ªå¯å¾®å‡½æ•°çš„å±€éƒ¨æœ€å°å€¼ã€‚å®ƒçš„æ€æƒ³æ˜¯åœ¨å½“å‰ç‚¹æ²¿ç€å‡½æ•°çš„è´Ÿæ¢¯åº¦æ–¹å‘ï¼ˆå³æœ€å¿«ä¸‹é™çš„æ–¹å‘ï¼‰é‡å¤åœ°èµ°ä¸€å°æ­¥ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥å‡å°‘å‡½æ•°çš„å€¼[^1] [^2]
æœ‰æ—¶å€™ï¼Œæ¢¯åº¦æ±‚é”™äº†ç»“æœåè€Œæ›´å¥½ï¼Œå¯èƒ½æ˜¯å› ä¸ºä½ é‡åˆ°äº†ä»¥ä¸‹æƒ…å†µä¹‹ä¸€ï¼š
>
> - å‡½æ•°æœ¬èº«ä¸æ˜¯å‡¸çš„ï¼Œæœ‰å¤šä¸ªå±€éƒ¨æœ€å°å€¼æˆ–éç‚¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ­£ç¡®çš„æ¢¯åº¦å¯èƒ½ä¼šå¯¼è‡´ä½ é™·å…¥ä¸€ä¸ªä¸ç†æƒ³çš„å±€éƒ¨æœ€å°å€¼æˆ–éç‚¹ï¼Œè€Œé”™è¯¯çš„æ¢¯åº¦å¯èƒ½ä¼šè®©ä½ è·³å‡ºå»ï¼Œæ‰¾åˆ°ä¸€ä¸ªæ›´å¥½çš„è§£[^2]ã€‚
> - æ­¥é•¿ï¼ˆæˆ–å­¦ä¹ ç‡ï¼‰å¤ªå¤§ï¼Œå¯¼è‡´ä½ åœ¨æœ€ä¼˜è§£é™„è¿‘éœ‡è¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ­£ç¡®çš„æ¢¯åº¦å¯èƒ½ä¼šè®©ä½ è¿œç¦»æœ€ä¼˜è§£ï¼Œè€Œé”™è¯¯çš„æ¢¯åº¦å¯èƒ½ä¼šè®©ä½ é è¿‘å®ƒ[^2]ã€‚
> - è¯¯å·®å‡½æ•°æœ‰å™ªå£°æˆ–éšæœºæ€§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ­£ç¡®çš„æ¢¯åº¦å¯èƒ½ä¸ç¨³å®šæˆ–ä¸å‡†ç¡®ï¼Œè€Œé”™è¯¯çš„æ¢¯åº¦å¯èƒ½æ°å¥½ä¸çœŸå®çš„æ¢¯åº¦æ¥è¿‘[^2]ã€‚
>  
> ä¸åŒçš„å‡½æ•°å’Œæ•°æ®è¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œå…¶ç»“æœå›å¤§æœ‰ä¸åŒ

## 3. ç¥ç»ç½‘ç»œ

### 3.1. CNN

- CNN å„å±‚å®é™…ä¸Šçš„è¾“å…¥çš„æ•°æ®çš„å¯è§†åŒ–:
  ![CNNå„å±‚å®é™…ä¸Šçš„è¾“å…¥çš„æ•°æ®çš„å¯è§†åŒ–](https://dsm04pap003files.storage.live.com/y4mCt4Y9P9o8Elp5VAPRz_b2WFM_8CY6spITt-fEO4liM9jr91ySE5RLX4YJQ1pR-sNFr_NkXqmEsZrq1NNMI_Bc0CJNn3tG74d2zk6STNLqiCm45_PbUQirpbSBjGpNNdx6jvKwi88Sa1dwb7fYrgnh901ke3BLMVxlUPDUhAdGrKA7w3uLtBfzeqk7cQvSlQn?width=993&height=783&cropmode=none)
  You can see it, with the increase in layers the detailed features of the images increase. At the very beginning, the images are just matrixes of pixels, but at the end, the images are matrixes of features. You can kinda recognize the objects in the image and eventually their identity is found by CNN.
- å¯¹å·ç§¯ç¥ç»ç½‘ç»œå¦‚ä½•å®ç°æ›´å¥½çš„ç»“æœçš„åŸå› ä»åœ¨ç ”ç©¶

## 4. è¡¨ç¤ºå­¦ä¹  (è¡¨å¾å­¦ä¹ )

é‡è¦çš„åŸå› :

- å¯¹äºå†—ä½™æ€§å¾ˆå¤§çš„æ•°æ®, æ¯”å¦‚ç…§ç‰‡çš„è¯†åˆ«, å°±æ˜¯ä¸€ä¸ªå¾ˆéœ€è¦è¡¨å¾å­¦ä¹ çš„å†…å®¹
- æˆ–è€…ä¾‹å¦‚å¦‚ä¸‹è¿™å¼ ç…§ç‰‡, çº¿æ€§å¯åˆ†çš„è¯æ˜¯ä¸éœ€è¦çš„, ä½†æ˜¯å¦‚æœä¸æ˜¯çº¿æ€§å¯åˆ†, å°±æ˜¯ç»å¸¸åˆ©ç”¨è¡¨è§‚å­¦ä¹ èƒ½æ›´å®¹æ˜“åœ°å¾—åˆ°æ›´å¥½çš„ç»“æœ

## 5. æ·±åº¦å­¦ä¹ 

### 5.1. æµ…å±‚å­¦ä¹  *shallow learning* VS. æ·±åº¦å­¦ä¹  *deep learning*

#### 5.1.1. shallow learning vs. deep learning table

- concisely:

|shallow learning|deep learning|
|:---:|:---:|
|linear model|non-linear model|
|feature engineering|feature learning|
|hand-crafted|end-to-end|
|local|global|
|small data|large data|
|fast|slow|
|simple|complex|
|low accuracy|high accuracy|

- diffusely:

Here is a table that compares shallow learning and deep learning based on some criteria[^3] [^4] [^5] [^6] [^7] [^8] [^9] [^10] [^11] [^12] [^13] [^14] [^15] [^16]:

| Criteria | Shallow Learning | Deep Learning |
| --- | --- | --- |
| Definition | Learning activities that characterized by recalling and rote memorization. The knowledge gained from shallow learning is considered passive and tends to fade away from our memory. | A type of machine learning based on artificial neural networks in which multiple layers of processing are used to extract progressively higher level features from data. |
| Number of hidden layers | One or none | More than one |
| Computational cost | Low | High |
| Interpretability | High | Low |
| Robustness to overfitting | High | Low |
| Scalability with data | Low, converges at a certain level of performance.  | High, improves with more data. |

#### 5.1.2. shallow learning models

- SIFT
- STIP
- SLAM

## 6. å‘å±•å†å²

![Brief History of AI](https://dsm04pap003files.storage.live.com/y4m55sWvC2yZZBn2ZZiW75vGyMRaOok8mmPTEu9JJyJQGc1eqQbzbK5_FCV-22Y30nizpIA4BYOt0FbWEjD2hhzRF6Tw82FZ3sgOC8EG1b3AjqgJFaXNaQoqGLQ9FP-mYX05pZjLGYr9ClNZWgTgWYnEjkajdEIXl1Lal0tfHcMmRcixv98ipN5N0VyFGpgjmq0?width=1711&height=400&cropmode=none)

- ç¥ç»å…ƒè¿›å…¥è®¡ç®—é¢†åŸŸçš„è§†é‡
- ç¥ç»ç½‘ç»œçš„ç¬¬ä¸€æ¬¡å¯’å†¬
- åå‘ä¼ æ’­ç®—æ³•(Backpropagation, BP)çš„å‡ºç°
- "é¦–ä¸ª" é‡è¦åº”ç”¨, æ‰‹å†™æ•°å­—è¯†åˆ«
- LeNet-5
- ç¥ç»ç½‘ç»œçš„ç¬¬äºŒæ¬¡å¯’å†¬
- å·â¼Ÿé‡æ¥çš„ï¼ˆæ·±åº¦ï¼‰ç¥ç»â½¹ç»œ
  åŸå› :
  - ä¼˜åŒ–ç­–ç•¥
    - æ¢¯åº¦å¼¥æ•£é—®é¢˜é€šè¿‡ReLUã€Dropoutã€Deep Residual Learningç­‰â½…æ³•å¾—åˆ°ç¼“è§£
  - æ•°æ®è§„æ¨¡
    - çœŸæ­£æ„ä¹‰ä¸Šçš„â¼¤æ•°æ®å‡ºç°==ç¤¾ä¼šæ ‡æ³¨(crowdsourcing)æœºåˆ¶==å‡ºç°
  - è®¡ç®—èƒ½â¼’
    - CPUã€GPUçš„â»“â¾œè¿›æ­¥
    - å…±äº«æƒå€¼
- AlexNet
- å¯å¤„ç†æ—¶åºæ•°æ®çš„æ·±åº¦å­¦ä¹ æœºåˆ¶
- å¤§ä¼ä¸šä¸åŒä»¥å¾€çš„å¯†åˆ‡å…³æ³¨
- å¼€æºæ¡†æ¶

## 7. DL related examples

### 7.1. The initial shock

æ¥æºäºè¯­éŸ³è¯†åˆ«

å¼€å§‹æ—¶çš„æ¨¡å‹æ˜¯GMM (Gaussian Mixture Models, é«˜æ–¯æ··åˆæ¨¡å‹), ä½†æ˜¯å½“æ·±åº¦å­¦ä¹ åˆ°æ¥ä¹‹å,ç”±äºå£°éŸ³æ˜¯ä¸€ä¸ªä¸€ç»´çš„æ•°æ®, é€‚åˆäºæ—©æœŸçš„æ·±åº¦å­¦ä¹ 

### 7.2. Object detection and sagmentation

Autodrive's Objection detection and sagmention already can segmente object in "3D way" now.

---

## 8. PS

å›å»çœ‹çœ‹ç»Ÿè®¡å­¦ä¹ 

â€”â€”èŠ±ä¹¦ Ch5

- å‰äº”ç« æ˜¯ML, è¦å…ˆè¯»æ‡‚æ, å¯„å¯„
- å¯„å¯„åŠ›, å¯„å¯„å­, è¦å¯„å¯„åŠ›, å‘œå‘œå‘œ

CS231n *FEIFEI LEE*, computer vision. suitable for selfteaching

to-do list:

- [x] ch1
- [ ] ch2
- [ ] ch3
- [ ] ch4
- [ ] ch5
- [ ] CS231n

## 9. Footnote

[^1]: Gradient descent - Wikipedia. <https://en.wikipedia.org/wiki/Gradient_descent> Accessed 2/24/2023.

[^2]: What is Gradient Descent? How does it work? - Medium. <https://medium.com/analytics-vidhya/what-is-gradient-descent-how-does-it-work-b713fab88349> Accessed 2/24/2023.

[^3]: What is Shallow Learning | IGI Global. <https://www.igi-global.com/dictionary/learning-with-immersive-technology/86355> Accessed 2/23/2023.

[^4]: What is the difference between deep learning and shallow learning?. <https://ai.stackexchange.com/questions/21219/what-is-the-difference-between-deep-learning-and-shallow-learning> Accessed 2/23/2023.

[^5]: <https://bing.com/search?q=deep+learning+definition> Accessed 2/23/2023.

[^6]: What is Deep Learning and How Does It Works [Updated] - Simplilearn.com. <https://www.simplilearn.com/tutorials/deep-learning-tutorial/what-is-deep-learning> Accessed 2/23/2023.

[^7]: Deep Neural Networks Vs Shallower Neural Networks: Advantages And .... <https://www.surfactants.net/deep-neural-networks-vs-shallower-neural-networks-advantages-and-disadvantages/> Accessed 2/23/2023.

[^8]: What Is Meant By Shallow Learning? â€“ TipsFolder.com. <https://tipsfolder.com/meant-shallow-learning-2d2fdc7d26f09cc856f911a271de233c/> Accessed 2/23/2023.

[^9]: Deep vs. shallow learning? | ResearchGate. <https://www.researchgate.net/post/Deep_vs_shallow_learning> Accessed 2/23/2023.

[^10]: What is the difference between deep learning and shallow learning .... <https://ai.stackexchange.com/questions/21219/what-is-the-difference-between-deep-learning-and-shallow-learning> Accessed 2/23/2023.

[^11]: Shallow Versus Deep Neural Networks - Coursera. <https://www.coursera.org/lecture/introduction-to-deep-learning-with-keras/shallow-versus-deep-neural-networks-3pKHn> Accessed 2/23/2023.

[^12]: What is Shallow Learning | IGI Global. <https://www.igi-global.com/dictionary/learning-with-immersive-technology/86355> Accessed 2/23/2023.

[^13]: Introduction to Shallow Machine Learning - LinkedIn. <https://www.linkedin.com/pulse/introduction-shallow-machine-learning-ayman-mahmoud> Accessed 2/23/2023.

[^14]: What is Deep Learning and How Does It Works [Updated] - Simplilearn.com. <https://www.simplilearn.com/tutorials/deep-learning-tutorial/what-is-deep-learning> Accessed 2/23/2023.

[^15]: Deep learning - Wikipedia. <https://en.wikipedia.org/wiki/Deep_learning> Accessed 2/23/2023.

[^16]: What is Deep Learning? | IBM. <https://www.ibm.com/topics/deep-learning> Accessed 2/23/2023.
