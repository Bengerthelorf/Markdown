> Given conditions:
> 
> > - (A1) The relationship between response ( $y$ ) and covariates ( $X$ ) is linear;
> > - (A2) $X$ is a non-stochastic matrix and rank( $X$ ) = $p$;
> > - (A3) $E(\epsilon) = 0$. This implies $E(y) = X \beta$;
> > - (A4) $cov( \epsilon ) = E( \epsilon \epsilon ^{\top} ) = \theta ^{2} I _{N}$; (Homoscedasticity)
> > - (A5) $\epsilon$ follows multivariate normal distribution $N(0, \epsilon ^{2} I _{N})$ (Normality)
>
> 
> Prove the following results:
> 
> $$
> \begin{align}
>     \hat{\beta} \sim N(\beta, \sigma ^{2} (X ^{\top} X) ^{-1} ) \tag{0.1} \\
>     (N - p) \hat{\sigma} ^{2} \sim \sigma ^{2} {X _{N-p}} ^{2}  \tag{0.2}
> \end{align}
> $$

To prove the results (0.1) and (0.2), we need to use the assumptions (A1) to (A5) and some properties of the normal distribution and the chi-squared distribution. Here are the steps for each proof:

## Proof of (0.1):

From the assumption (A1), we have $y = X\beta + \epsilon$, where $y$ is an $N \times 1$ vector of responses, $X$ is an $N \times p$ matrix of covariates, $\beta$ is a $p \times 1$ vector of parameters, and $\epsilon$ is an $N \times 1$ vector of errors.

The OLS estimator of $\beta$ is given by $\hat{\beta} = (X'X)^{-1}X'y$, where $X'$ is the transpose of $X$.

Substituting the model equation into the OLS estimator, we get $\hat{\beta} = (X'X)^{-1}X'(X\beta + \epsilon) = \beta + (X'X)^{-1}X'\epsilon$.

Taking the expectation of both sides, we get $E(\hat{\beta}) = E(\beta + (X'X)^{-1}X'\epsilon) = E(\beta) + E((X'X)^{-1}X'\epsilon)$.

Using the linearity of expectation and the assumption (A3), we get $E(\hat{\beta}) = \beta + (X'X)^{-1}X'E(\epsilon) = \beta + (X'X)^{-1}X'(0) = \beta$. This shows that $\hat{\beta}$ is an unbiased estimator of $\beta$.

Taking the variance of both sides, we get $Var(\hat{\beta}) = Var(\beta + (X'X)^{-1}X'\epsilon) = Var((X'X)^{-1}X'\epsilon)$, where we used the fact that $\beta$ is a constant and has zero variance.

Using the property that $Var(AX) = AVar(X)A'$ for any matrix $A$ and vector $X$, we get $Var(\hat{\beta}) = (X'X)^{-1}X'Var(\epsilon)X(X'X)^{-1}$.

Using the assumption (A4), we get $Var(\hat{\beta}) = (X'X)^{-1}X'\sigma^2 I_N X(X'X)^{-1} = \sigma^2(X'X)^{-1}$, where $I_N$ is the identity matrix of size $N$.

Using the assumption (A5), we get that $\epsilon \sim N(0, \sigma^2 I_N)$, which implies that $(X'X)^{-1}X'\epsilon \sim N(0, \sigma^2(X'X)^{-1})$, since a linear transformation of a normal vector is also normal with mean and variance given by the transformation.

Adding $\beta$ to both sides, we get that $\hat{\beta} \sim N(\beta, \sigma^2(X'X)^{-1})$, which proves the result (0.1).

## Proof of (0.2):

The OLS estimator of $\sigma^2$ is given by $\hat{\sigma}^2 = \frac{1}{N - p}\sum_{i=1}^N e_i^2$, where $e_i$ is the residual for observation $i$, defined as $e_i = y_i - x_i'\hat{\beta}$, where $x_i'$ is the transpose of the $i$th row of $X$.

Substituting the model equation into the residual, we get $e_i = y_i - x_i'\hat{\beta} = x_i'\beta + \epsilon_i - x_i'\hat{\beta} = x_i'(\beta - \hat{\beta}) + \epsilon_i$.

Using matrix notation, we can write the vector of residuals as $e = y - X\hat{\beta} = X\beta + \epsilon - X\hat{\beta} = X(\beta - \hat{\beta}) + \epsilon$. 

Premultiplying both sides by $(I_N - X(X'X)^{-1}X')$, where $I_N$ is the identity matrix of size $N$, we get $(I_N - X(X'X)^{-1}X')e = (I_N - X(X'X)^{-1}X')( X(\beta - \hat{\beta}) + \epsilon)$.

Simplifying, we get $(I_N - X(X'X)^{-1}X')e = (I_N - X(X'X)^{-1}X')\epsilon$, since $(I_N - X(X'X)^{-1}X')X = X - X = 0$.

Taking the norm of both sides, we get $\| (I_N - X(X'X)^{-1}X')e \| = \| (I_N - X(X'X)^{-1}X')\epsilon \|$.

Squaring both sides, we get $e'(I_N - X(X'X)^{-1}X')e = \epsilon'(I_N - X(X'X)^{-1}X')\epsilon$, where $e'$ and $\epsilon'$ are the transposes of $e$ and $\epsilon$ respectively.

Using the property that $(I_N - X(X'X)^{-1}X')$ is a symmetric and idempotent matrix, we get $e'(I_N - X(X'X)^{-1}X')e = e'e = \epsilon'(I_N - X(X'X)^{-1}X')\epsilon = \epsilon'\epsilon$.

Dividing both sides by $(N - p)\sigma^2$, we get $\frac{1}{(N - p)\sigma^2}\sum_{i=1}^N e_i^2 = \frac{1}{(N - p)\sigma^2}\sum_{i=1}^N \epsilon_i^2$.

Using the definition of $\hat{\sigma}^2$, we get $\frac{(N - p)\hat{\sigma}^2}{\sigma^2} = \frac{1}{\sigma^2}\sum_{i=1}^N \epsilon_i^2$.

Using the assumption (A5), we get that $\epsilon_i \sim N(0, \sigma^2)$, which implies that $\frac{\epsilon_i^2}{\sigma^2} \sim \chi_1^2$, where $\chi_1^2$ is the chi-squared distribution with one degree of freedom, since the square of a standard normal variable follows a chi-squared distribution with one degree of freedom.

Adding $N$ independent chi-squared variables, we get that $\frac{1}{\sigma^2}\sum_{i=1}^N \epsilon_i^2 \sim \chi_N^2$, where $\chi_N^2$ is the chi-squared distribution with $N$ degrees of freedom, since the sum of independent chi-squared variables follows a chi-squared distribution with degrees of freedom equal to the sum of the individual degrees of freedom.

Using the property that a linear transformation of a chi-squared variable is also chi-squared, we get that $\frac{(N - p)\hat{\sigma}^2}{\sigma^2} \sim \chi_{N-p}^2$, where $\chi_{N-p}^2$ is the chi-squared distribution with $N - p$ degrees of freedom, since multiplying by a constant $(N - p)$ does not change the shape of the distribution, but only scales it by the same factor.

This proves the result (0.2).