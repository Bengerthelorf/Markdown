> Prove the Gauss-Markov Theorem.

高斯-马尔可夫定理指出，在一定的假设下，普通最小二乘法（OLS）估计量是真实参数的最优线性无偏估计量（BLUE）。要证明这个定理，我们需要证明OLS估计量在所有线性无偏估计量中具有最小的方差。

证明如下：

设线性回归模型为$y = X\beta + \epsilon$，其中$y$是$n \times 1$的因变量向量，$X$是$n \times k$的自变量矩阵，$\beta$是$k \times 1$的参数向量，$\epsilon$是$n \times 1$的误差向量。假设$\epsilon$满足以下条件：

- 期望为零：$E(\epsilon) = 0$
- 同方差：$Var(\epsilon) = \sigma^2 I_n$
- 无相关：$Cov(\epsilon_i, \epsilon_j) = 0, i \neq j$

OLS估计量为$\hat{\beta} = (X'X)^{-1}X'y$，它是$\beta$的一个线性无偏估计量，即$\hat{\beta} = Cy$，其中$C = (X'X)^{-1}X'$，且满足$E(\hat{\beta}) = CX\beta = \beta$。

设$\tilde{\beta}$是$\beta$的另一个线性无偏估计量，即$\tilde{\beta} = Dy$，其中$D$是一个$n \times k$的矩阵，且满足$E(\tilde{\beta}) = DX\beta = \beta$。这意味着$(D - C)X = 0$。

我们要比较$\hat{\beta}$和$\tilde{\beta}$的方差。由于它们都是$k \times 1$的向量，我们可以用它们的方差-协方差矩阵来表示。注意到方差-协方差矩阵是半正定的，即对于任意非零向量$a$，都有$a'Va \geq 0$。因此，我们可以用半正定性来判断两个估计量的方差大小。

具体地，我们有：

$$Var(\hat{\beta}) = CVar(y)C' = C\sigma^2 I_n C' = \sigma^2 CC'$$

$$Var(\tilde{\beta}) = DVar(y)D' = D\sigma^2 I_n D' = \sigma^2 DD'$$

$$Var(\tilde{\beta}) - Var(\hat{\beta}) = \sigma^2 (DD' - CC')$$

为了证明OLS估计量是BLUE，我们需要证明对于任意非零向量$a$，都有：

$$a'(Var(\tilde{\beta}) - Var(\hat{\beta}))a \geq 0$$

即：

$$a'\sigma^2 (DD' - CC')a \geq 0$$

由于$\sigma^2 > 0$，我们可以除以它而不改变不等号的方向。因此，我们只需要证明：

$$a'(DD' - CC')a \geq 0$$

展开后，我们得到：

$$a'DD'a - a'CC'a \geq 0$$

令$b = Da - Ca$，则上式等价于：

$$b'b - a'CC'a \geq 0$$

即：

$$b'b \geq a'CC'a$$

由于$b'b \geq 0$且$a'CC'a \geq 0$，我们可以对两边开平方而不改变不等号的方向。因此，我们只需要证明：

$$||b|| \geq ||Ca||$$

其中$||\cdot||$表示向量的欧几里得范数。注意到$Ca$是$X$的列空间中的一个向量，而$b = Da - Ca$是$(D - C)X$的列空间中的一个向量。由于$(D - C)X = 0$，这意味着$b$是$X$的零空间中的一个向量。根据勾股定理，我们有：

$$||b + Ca||^2 = ||b||^2 + ||Ca||^2$$

由于$b + Ca = Da$，我们可以写成：

$$||Da||^2 = ||b||^2 + ||Ca||^2$$

两边开平方，我们得到：

$$||Da|| = \sqrt{||b||^2 + ||Ca||^2}$$

由于平方根函数是单调递增的，我们可以对两边平方而不改变不等号的方向。因此，我们只需要证明：

$$||Da||^2 \geq ||b||^2 + ||Ca||^2$$

展开后，我们得到：

$$a'DD'a \geq a'(D - C)'(D - C)a + a'CC'a$$

化简后，我们得到：

$$a'(DD' - CC')a \geq 0$$

这正是我们要证明的不等式。因此，我们证明了OLS估计量的方差-协方差矩阵小于或等于任何其他线性无偏估计量的方差-协方差矩阵。这就完成了高斯-马尔可夫定理的证明。