> 1. Prove that the OLS estimator $\hat{\beta}$ is the same as the maximum likelihood estimator.


要证明OLS估计量$\hat{\beta}$与最大似然估计量相同，我们需要假设误差项$\epsilon$服从均值为零，方差为$\sigma^2$的正态分布，即$\epsilon \sim N(0, \sigma^2)$。这意味着因变量$y$也服从均值为$X\beta$，方差为$\sigma^2$的正态分布，即$y \sim N(X\beta, \sigma^2)$。

给定$X$和$\beta$时，$y$的似然函数为：

$$L(y|X,\beta) = \prod_{i=1}^n f(y_i|x_i,\beta)$$

其中$f(y_i|x_i,\beta)$是正态分布的概率密度函数在$y_i$处的取值。代入正态密度的公式，我们得到：

$$L(y|X,\beta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - x_i'\beta)^2}{2\sigma^2}\right)$$

对两边取自然对数，我们得到对数似然函数：

$$\log L(y|X,\beta) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - x_i'\beta)^2$$

要找到$\beta$的最大似然估计量，我们需要对$\beta$最大化对数似然函数。求一阶导数并令其为零，我们得到：

$$\frac{\partial \log L(y|X,\beta)}{\partial \beta} = \frac{1}{\sigma^2} \sum_{i=1}^n x_i (y_i - x_i'\beta) = 0$$

化简并重新排列，我们得到：

$$\sum_{i=1}^n x_i y_i = \sum_{i=1}^n x_i x_i' \beta$$

用矩阵表示法，这可以写成：

$$X'y = X'X \beta$$

解出$\beta$，我们得到：

$$\hat{\beta}_{MLE} = (X'X)^{-1} X'y$$

这与OLS估计量$\hat{\beta}$完全相同。因此，我们证明了OLS估计量$\hat{\beta}$与最大似然估计量相同。