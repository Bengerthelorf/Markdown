# 2nd

## LMs

$$
\left\{
\begin{aligned}
\begin{align*}
    \text{LM: } V &= \{v_1, v_2, \cdots, v_k, \cdots v_M\} \\
    &= \{'one', 'two', \cdots, 'the', \cdots\} \\
\end{align*} \\
\begin{align*}
    \text{LM: } V &= \{v_1, v_2, \cdots, v_k, \cdots v_M\} \\
    &= \{'one', 'two', \cdots, 'the', \cdots\} \\
\end{align*}
\end{aligned}
\right.
$$

当然，让我们来看一个简化的例子来说明没有EOS时概率计算的问题。

假设我们有一个非常简单的词汇表 \( V = \{w_1, w_2, EOS\} \)，其中 \( w_1 \) 和 \( w_2 \) 是词汇，EOS是序列结束标志。我们的模型是一个简单的bigram模型，它只考虑相邻两个词的关系。下面是这个模型的概率表：

\[
\begin{array}{c|c|c}
 & w_1 & w_2 & EOS \\
\hline
w_1 & 0.3 & 0.3 & 0.4 \\
w_2 & 0.3 & 0.3 & 0.4 \\
EOS & 0 & 0 & 0 \\
\end{array}
\]

在这个表中，\( P(w_2 | w_1) = 0.3 \) 表示如果当前词是 \( w_1 \)，下一个词是 \( w_2 \) 的概率是0.3。同样，\( P(EOS | w_1) = 0.4 \) 表示从 \( w_1 \) 结束序列的概率是0.4。

现在，如果我们要计算一个序列的概率，比如 \( w_1 w_2 EOS \)，我们会这样计算：

\[
P(w_1 w_2 EOS) = P(w_1) \times P(w_2 | w_1) \times P(EOS | w_2)
\]

假设每个词在序列开始时出现的概率是均等的，即 \( P(w_1) = P(w_2) = \frac{1}{2} \)，那么：

\[
P(w_1 w_2 EOS) = \frac{1}{2} \times 0.3 \times 0.4 = 0.06
\]

这意味着序列 \( w_1 w_2 EOS \) 出现的概率是0.06。

如果没有EOS，我们的模型可能会继续预测下一个词，而不会停止。这会导致所有可能序列的概率之和超过1，这是不可能的，因为概率分布的总和必须是1。这就是为什么EOS在模型中非常重要的原因。

希望这个例子能帮助您理解EOS在概率计算中的作用。如果您有任何其他问题，欢迎继续提问！

> P.S. 拉格朗日乘子法
>
> 拉格朗日乘子法是一种在数学优化中寻找多元函数在约束条件下极值的方法。通过引入拉格朗日乘子（一个或多个新的标量未知数），可以将有约束的优化问题转化为无约束优化问题来求解。具体来说，假设我们要在某个约束 \( g(\mathbf{x}) = 0 \) 下，找到函数 \( f(\mathbf{x}) \) 的极值。我们可以构造拉格朗日函数 \( L(\mathbf{x}, \lambda) = f(\mathbf{x}) + \lambda g(\mathbf{x}) \)，其中 \( \lambda \) 是拉格朗日乘子。然后，通过对 \( L(\mathbf{x}, \lambda) \) 求偏导并令其为零，可以求解出满足条件的极值点。

对于验证集, 当不是对于0/1的布尔式的时候, 我们可以用对model之间预测的可能性的大小的对比来作为评估指标

$$
P_{A (w_1, w_2 \cdots w_n)} \\
P_{B (w_1, w_2 \cdots w_n)}
$$

当 \[P_A > P_B\] 的时候, 我们可以认为模型A的预测更好

## Perplexity

评估新语言模型的方法有很多种，其中一些是基于人类专家的评估，而其他一些则基于自动化评估。这些方法各有优缺点，本文就来介绍基于自动化评估的困惑度方法。

困惑度（Perplexity）是一种用于评估语言模型好坏的指标，它可以衡量一个语言模型在给定一组数据时的预测能力。困惑度的值越小，表示模型的预测能力越好。常被应用于评估自然语言处理模型，它是衡量模型在给定文本中预测下一个单词的能力的指标。较低的困惑度表示更好的模型性能。

在自然语言处理中，语言模型的目的是预测一个序列中下一个单词的出现概率。给定一个单词序列$w_1,w_2,…,w_n$，语言模型的目标是计算该序列的联合概率P(w_1,w_2,…,w_n)。使用链式法则，可以将联合概率分解为条件概率的乘积：$P(w_1,w_2,…,w_n)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)…P(w_n|w_1,w_2,…,w_{n-1})$

困惑度是指在计算条件概率时，使用模型预测的概率分布对应的熵的大小。具体来说，对于给定的测试数据集D，困惑度可以定义为：$perplexity(D)=\sqrt[N]{\prod_{i=1}^{N}\frac{1}{P(w_i|w_1,w_2,…,w_{i-1})}}$

其中，N是数据集D中单词的总数。$P(w_i|w_1,w_2,…,w_{i-1})$是给定前i-1个单词时，模型预测第i个单词的条件概率。困惑度的值越小，表示模型的预测能力越好。

困惑度的原理
困惑度的原理是基于信息熵的概念。信息熵是一个随机变量的不确定性的度量，它表示对于一个离散随机变量X，其熵的定义为：$H(X)=-\sum_{x}P(x)\log P(x)$

其中，P(x)是随机变量X取值为x的概率。熵越大，表示随机变量的不确定性越高。

在语言模型中，困惑度的计算可以转化为对给定测试数据集D中每个单词的条件概率的熵值求和的平均值。困惑度的值越小，表示模型预测的概率分布越接近真实的概率分布，模型的表现越好。

困惑度的实现方法
在实现困惑度的计算时，需要使用训练好的语言模型对测试数据集中的每个单词的条件概率进行预测。具体来说，可以使用以下步骤计算困惑度：

对测试数据集中的每个单词，使用已训练好的语言模型计算其条件概率$P(w_i|w_1,w_2,…,w_{i-1})$。

对每个单词的条件概率取对数，以避免概率的乘积变成概率的和之后下溢或者产生误差。计算公式为：$\log P(w_i|w_1,w_2,…,w_{i-1})$

将每个单词的条件概率对数的负数相加，得到测试数据集的困惑度。计算公式为：perplexity(D)=\exp\left{-\frac{1}{N}\sum_{i=1}^{N}\log P(w_i|w_1,w_2,…,w_{i-1})\right}

困惑度的计算需要使用已训练好的语言模型，因此在实现时需要先训练好语言模型。训练语言模型的方法有很多种，例如n-gram模型、神经网络语言模型等。在训练时，需要使用一个大规模的文本语料库，以便模型能够学习到单词之间的关系和概率分布。

总的来说，困惑度是一种常用的评估语言模型好坏的指标。通过计算测试数据集中每个单词的条件概率的熵值求和的平均值，可以评估语言模型的预测能力。困惑度越小，表示模型预测的概率分布越接近真实的概率分布，模型的表现越好。

> 交叉熵核心劣势:
