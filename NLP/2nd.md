# 2nd

## LMs

$$
\left\{
\begin{aligned}
\begin{align*}
    \text{LM: } V &= \{v_1, v_2, \cdots, v_k, \cdots v_M\} \\
    &= \{'one', 'two', \cdots, 'the', \cdots\} \\
\end{align*} \\
\begin{align*}
    \text{LM: } V &= \{v_1, v_2, \cdots, v_k, \cdots v_M\} \\
    &= \{'one', 'two', \cdots, 'the', \cdots\} \\
\end{align*}
\end{aligned}
\right.
$$

当然，让我们来看一个简化的例子来说明没有EOS时概率计算的问题。

假设我们有一个非常简单的词汇表 \( V = \{w_1, w_2, EOS\} \)，其中 \( w_1 \) 和 \( w_2 \) 是词汇，EOS是序列结束标志。我们的模型是一个简单的bigram模型，它只考虑相邻两个词的关系。下面是这个模型的概率表：

\[
\begin{array}{c|c|c}
 & w_1 & w_2 & EOS \\
\hline
w_1 & 0.3 & 0.3 & 0.4 \\
w_2 & 0.3 & 0.3 & 0.4 \\
EOS & 0 & 0 & 0 \\
\end{array}
\]

在这个表中，\( P(w_2 | w_1) = 0.3 \) 表示如果当前词是 \( w_1 \)，下一个词是 \( w_2 \) 的概率是0.3。同样，\( P(EOS | w_1) = 0.4 \) 表示从 \( w_1 \) 结束序列的概率是0.4。

现在，如果我们要计算一个序列的概率，比如 \( w_1 w_2 EOS \)，我们会这样计算：

\[
P(w_1 w_2 EOS) = P(w_1) \times P(w_2 | w_1) \times P(EOS | w_2)
\]

假设每个词在序列开始时出现的概率是均等的，即 \( P(w_1) = P(w_2) = \frac{1}{2} \)，那么：

\[
P(w_1 w_2 EOS) = \frac{1}{2} \times 0.3 \times 0.4 = 0.06
\]

这意味着序列 \( w_1 w_2 EOS \) 出现的概率是0.06。

如果没有EOS，我们的模型可能会继续预测下一个词，而不会停止。这会导致所有可能序列的概率之和超过1，这是不可能的，因为概率分布的总和必须是1。这就是为什么EOS在模型中非常重要的原因。

希望这个例子能帮助您理解EOS在概率计算中的作用。如果您有任何其他问题，欢迎继续提问！
